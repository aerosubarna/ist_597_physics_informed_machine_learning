{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkLSfB-XKFG3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        self.a = self.activation_function(self.z)\n",
        "        return self.a"
      ],
      "metadata": {
        "id": "zOrkgVQrKLmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data"
      ],
      "metadata": {
        "id": "IngBVpXoKU5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "x = np.random.rand()\n",
        "y = np.random.rand()\n",
        "\n",
        "input_vector = np.array([x, y]).reshape(2, 1)\n",
        "output_vector = nn.forward(input_vector)\n",
        "print(output_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3eMtR1fKaoT",
        "outputId": "110736ad-5457-406d-d3b7-1f0af8d357ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.09409977]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward mode ad\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        self.n_params = output_size * input_size + output_size\n",
        "\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)\n",
        "\n",
        "    def forward(self, input_data, param_index=None):\n",
        "        self.input_data = input_data\n",
        "        batch_size = input_data.shape[1]\n",
        "\n",
        "        # Initialize derivatives\n",
        "        w_size = self.output_size * self.input_size\n",
        "        if param_index is not None:\n",
        "            # Create derivative matrices\n",
        "            dw = np.zeros_like(self.weights)\n",
        "            db = np.zeros_like(self.biases)\n",
        "\n",
        "            if param_index < w_size:\n",
        "                # This is a weight parameter\n",
        "                i, j = param_index // self.input_size, param_index % self.input_size\n",
        "                dw[i, j] = 1.0\n",
        "            else:\n",
        "                # This is a bias parameter\n",
        "                bias_idx = param_index - w_size\n",
        "                db[bias_idx] = 1.0\n",
        "\n",
        "        # Forward pass\n",
        "        z = np.dot(self.weights, input_data) + self.biases\n",
        "        a = self.activation_function(z)\n",
        "\n",
        "        if param_index is not None:\n",
        "            # Compute derivative of z\n",
        "            dz = np.dot(dw, input_data) + db\n",
        "            # Compute derivative of activation\n",
        "            da = self.activation_derivative(z) * dz\n",
        "            return a, da\n",
        "        return a, None\n",
        "\n",
        "    def update_parameters(self, learning_rate, gradients):\n",
        "        w_size = self.output_size * self.input_size\n",
        "\n",
        "        # Reshape gradients for weights and biases\n",
        "        dW = gradients[:w_size].reshape(self.output_size, self.input_size)\n",
        "        db = gradients[w_size:].reshape(self.output_size, 1)\n",
        "\n",
        "        # Update parameters\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data, _ = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def get_total_params(self):\n",
        "        return sum(layer.n_params for layer in self.layers)\n",
        "\n",
        "    def compute_gradients(self, X, Y):\n",
        "        n_total_params = self.get_total_params()\n",
        "        gradients = []\n",
        "\n",
        "        # For each parameter in the network\n",
        "        param_count = 0\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            for param_idx in range(layer.n_params):\n",
        "                # Forward pass with derivative with respect to this parameter\n",
        "                input_data = X\n",
        "                param_derivative = None\n",
        "\n",
        "                for i, current_layer in enumerate(self.layers):\n",
        "                    if i == layer_idx:\n",
        "                        input_data, param_derivative = current_layer.forward(input_data, param_idx)\n",
        "                    else:\n",
        "                        input_data, _ = current_layer.forward(input_data)\n",
        "\n",
        "                # Compute loss derivative\n",
        "                m = Y.shape[1]\n",
        "                output_derivative = (input_data - Y) / m\n",
        "\n",
        "                # Compute gradient for this parameter\n",
        "                if param_derivative is not None:\n",
        "                    grad = np.sum(output_derivative * param_derivative)\n",
        "                    gradients.append(grad)\n",
        "\n",
        "                param_count += 1\n",
        "\n",
        "        return np.array(gradients)"
      ],
      "metadata": {
        "id": "pV1XiaHvKeIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "x = np.random.rand()\n",
        "y = np.random.rand()\n",
        "\n",
        "input_vector = np.array([x, y]).reshape(2, 1)\n",
        "output_vector = nn.forward(input_vector)\n",
        "gradients = nn.compute_gradients(input_vector, output_vector)"
      ],
      "metadata": {
        "id": "hTsGxvopLNkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average time to compute gradients\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(1000):\n",
        "  nn = NeuralNetwork(input_size=2)\n",
        "  nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "  nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "  nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "  x = np.random.rand()\n",
        "  y = np.random.rand()\n",
        "\n",
        "  input_vector = np.array([x, y]).reshape(2, 1)\n",
        "  output_vector = nn.forward(input_vector)\n",
        "  gradients = nn.compute_gradients(input_vector, output_vector)\n",
        "end_time = time.time()\n",
        "print(\"Average Time:\" + str((end_time - start_time) / 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDA7FA2GWO2F",
        "outputId": "2d9e4413-0aba-4974-988f-9a980965dd63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Time:0.006320048332214356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mode AD\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x  # Linear activation\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        self.a = self.activation_function(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, dA):\n",
        "        m = self.input_data.shape[1]  # Batch size\n",
        "        self.dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
        "        self.dW = np.dot(self.dZ, self.input_data.T) / m\n",
        "        self.db = np.sum(self.dZ, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(self.weights.T, self.dZ)\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def compute_gradients(self, Y, learning_rate):\n",
        "        m = Y.shape[1]\n",
        "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA)"
      ],
      "metadata": {
        "id": "WObg-ueqLOcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "x = np.random.rand()\n",
        "y = np.random.rand()\n",
        "\n",
        "input_vector = np.array([x, y]).reshape(2, 1)\n",
        "output_vector = nn.forward(input_vector)\n",
        "nn.compute_gradients(output_vector, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "yIeV3dDDNjqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average time\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "for i in range(1000):\n",
        "  nn = NeuralNetwork(input_size=2)\n",
        "  nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "  nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "  nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "  x = np.random.rand()\n",
        "  y = np.random.rand()\n",
        "\n",
        "  input_vector = np.array([x, y]).reshape(2, 1)\n",
        "  output_vector = nn.forward(input_vector)\n",
        "  nn.compute_gradients(output_vector, learning_rate=0.01)\n",
        "end_time = time.time()\n",
        "print(\"Average Time:\" + str((end_time - start_time) / 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ujb69SbW4Wt",
        "outputId": "fa32c716-286e-471b-a492-c2341c00ce42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Time:0.00015849041938781737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse mode training\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x  # Linear activation\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        self.a = self.activation_function(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, dA, learning_rate):\n",
        "        m = self.input_data.shape[1]  # Batch size\n",
        "        dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
        "        dW = np.dot(dZ, self.input_data.T) / m\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        # Update weights and biases using SGD\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def backward(self, Y, learning_rate):\n",
        "        m = Y.shape[1]\n",
        "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA, learning_rate)\n",
        "\n",
        "    def compute_loss(self, predictions, Y):\n",
        "        m = Y.shape[1]\n",
        "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(output, Y)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(Y, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")"
      ],
      "metadata": {
        "id": "hRz7Bog9OIA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(num_samples=1000):\n",
        "    x = np.random.uniform(0, 1, num_samples)\n",
        "    y = np.random.uniform(0, 1, num_samples)\n",
        "    f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
        "    return np.vstack((x, y)), f_xy.reshape(1, -1)\n",
        "\n",
        "# Define the neural network\n",
        "nn = NeuralNetwork(input_size=2)\n",
        "\n",
        "# Add layers: 3 hidden layers with 10 neurons each, ReLU activation\n",
        "nn.add_layer(Layer(2, 10, activation='relu'))\n",
        "nn.add_layer(Layer(10, 10, activation='relu'))\n",
        "nn.add_layer(Layer(10, 10, activation='relu'))\n",
        "\n",
        "# Output layer with 1 neuron, no activation (linear output)\n",
        "nn.add_layer(Layer(10, 1, activation='linear'))\n",
        "\n",
        "# Training the network\n",
        "X, Y = generate_data(1000)  # Generate 1000 samples\n",
        "epochs = 5000\n",
        "learning_rate = 0.01\n",
        "\n",
        "nn.train(X, Y, epochs, learning_rate)\n",
        "\n",
        "# Test the network\n",
        "test_X, test_Y = generate_data(100)  # Generate test data\n",
        "predictions = nn.forward(test_X)\n",
        "\n",
        "# Compute test loss\n",
        "test_loss = nn.compute_loss(predictions, test_Y)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoxS6d5rTJFj",
        "outputId": "be9c3b72-408b-4c80-86b6-716a60ee0a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000 complete, Loss: 0.2271385636525113\n",
            "Epoch 100/5000 complete, Loss: 0.07404594731873411\n",
            "Epoch 200/5000 complete, Loss: 0.0711372334580047\n",
            "Epoch 300/5000 complete, Loss: 0.0700151479142721\n",
            "Epoch 400/5000 complete, Loss: 0.06921742021911749\n",
            "Epoch 500/5000 complete, Loss: 0.06850878051565168\n",
            "Epoch 600/5000 complete, Loss: 0.06784962641899123\n",
            "Epoch 700/5000 complete, Loss: 0.06729003899454157\n",
            "Epoch 800/5000 complete, Loss: 0.06678812180843348\n",
            "Epoch 900/5000 complete, Loss: 0.06633584935339971\n",
            "Epoch 1000/5000 complete, Loss: 0.06592175858575816\n",
            "Epoch 1100/5000 complete, Loss: 0.06553801177884182\n",
            "Epoch 1200/5000 complete, Loss: 0.06517746579376489\n",
            "Epoch 1300/5000 complete, Loss: 0.06483152278946122\n",
            "Epoch 1400/5000 complete, Loss: 0.06448471301126978\n",
            "Epoch 1500/5000 complete, Loss: 0.06414060234570222\n",
            "Epoch 1600/5000 complete, Loss: 0.06381750133575206\n",
            "Epoch 1700/5000 complete, Loss: 0.06349531827374334\n",
            "Epoch 1800/5000 complete, Loss: 0.0631880571496179\n",
            "Epoch 1900/5000 complete, Loss: 0.06288029171697214\n",
            "Epoch 2000/5000 complete, Loss: 0.06257818065114046\n",
            "Epoch 2100/5000 complete, Loss: 0.06228598032236438\n",
            "Epoch 2200/5000 complete, Loss: 0.061991210996375584\n",
            "Epoch 2300/5000 complete, Loss: 0.06169418559266337\n",
            "Epoch 2400/5000 complete, Loss: 0.061395733396520644\n",
            "Epoch 2500/5000 complete, Loss: 0.061093157856430665\n",
            "Epoch 2600/5000 complete, Loss: 0.060791115753951575\n",
            "Epoch 2700/5000 complete, Loss: 0.06048754080381495\n",
            "Epoch 2800/5000 complete, Loss: 0.06017739665931415\n",
            "Epoch 2900/5000 complete, Loss: 0.059859281069065524\n",
            "Epoch 3000/5000 complete, Loss: 0.05953584588685247\n",
            "Epoch 3100/5000 complete, Loss: 0.05920377455984482\n",
            "Epoch 3200/5000 complete, Loss: 0.05886426901017622\n",
            "Epoch 3300/5000 complete, Loss: 0.058518110040964834\n",
            "Epoch 3400/5000 complete, Loss: 0.05816314597329341\n",
            "Epoch 3500/5000 complete, Loss: 0.057796110781584135\n",
            "Epoch 3600/5000 complete, Loss: 0.05741229647595245\n",
            "Epoch 3700/5000 complete, Loss: 0.05701537367772373\n",
            "Epoch 3800/5000 complete, Loss: 0.05659907128097683\n",
            "Epoch 3900/5000 complete, Loss: 0.05616002321926254\n",
            "Epoch 4000/5000 complete, Loss: 0.0556930164392935\n",
            "Epoch 4100/5000 complete, Loss: 0.05519635293118285\n",
            "Epoch 4200/5000 complete, Loss: 0.05466705263255308\n",
            "Epoch 4300/5000 complete, Loss: 0.05410737439850597\n",
            "Epoch 4400/5000 complete, Loss: 0.053508126868583916\n",
            "Epoch 4500/5000 complete, Loss: 0.052884980722463636\n",
            "Epoch 4600/5000 complete, Loss: 0.05223476873991378\n",
            "Epoch 4700/5000 complete, Loss: 0.051546593590309414\n",
            "Epoch 4800/5000 complete, Loss: 0.05083369007852944\n",
            "Epoch 4900/5000 complete, Loss: 0.050092495883153274\n",
            "Test Loss: 0.05546869464729145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# forward mode training\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Total number of parameters\n",
        "        self.n_params = output_size * input_size + output_size\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)\n",
        "\n",
        "    def forward(self, input_data, param_index=None):\n",
        "        self.input_data = input_data\n",
        "        batch_size = input_data.shape[1]\n",
        "\n",
        "        # Initialize derivatives\n",
        "        w_size = self.output_size * self.input_size\n",
        "        if param_index is not None:\n",
        "            # Create derivative matrices\n",
        "            dw = np.zeros_like(self.weights)\n",
        "            db = np.zeros_like(self.biases)\n",
        "\n",
        "            if param_index < w_size:\n",
        "                # This is a weight parameter\n",
        "                i, j = param_index // self.input_size, param_index % self.input_size\n",
        "                dw[i, j] = 1.0\n",
        "            else:\n",
        "                # This is a bias parameter\n",
        "                bias_idx = param_index - w_size\n",
        "                db[bias_idx] = 1.0\n",
        "\n",
        "        # Forward pass\n",
        "        z = np.dot(self.weights, input_data) + self.biases\n",
        "        a = self.activation_function(z)\n",
        "\n",
        "        if param_index is not None:\n",
        "            # Compute derivative of z\n",
        "            dz = np.dot(dw, input_data) + db\n",
        "            # Compute derivative of activation\n",
        "            da = self.activation_derivative(z) * dz\n",
        "            return a, da\n",
        "        return a, None\n",
        "\n",
        "    def update_parameters(self, learning_rate, gradients):\n",
        "        w_size = self.output_size * self.input_size\n",
        "\n",
        "        # Reshape gradients for weights and biases\n",
        "        dW = gradients[:w_size].reshape(self.output_size, self.input_size)\n",
        "        db = gradients[w_size:].reshape(self.output_size, 1)\n",
        "\n",
        "        # Update parameters\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data, _ = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def get_total_params(self):\n",
        "        return sum(layer.n_params for layer in self.layers)\n",
        "\n",
        "    def compute_gradients(self, X, Y):\n",
        "        n_total_params = self.get_total_params()\n",
        "        gradients = []\n",
        "\n",
        "        # For each parameter in the network\n",
        "        param_count = 0\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            for param_idx in range(layer.n_params):\n",
        "                # Forward pass with derivative with respect to this parameter\n",
        "                input_data = X\n",
        "                param_derivative = None\n",
        "\n",
        "                for i, current_layer in enumerate(self.layers):\n",
        "                    if i == layer_idx:\n",
        "                        input_data, param_derivative = current_layer.forward(input_data, param_idx)\n",
        "                    else:\n",
        "                        input_data, _ = current_layer.forward(input_data)\n",
        "\n",
        "                # Compute loss derivative\n",
        "                m = Y.shape[1]\n",
        "                output_derivative = (input_data - Y) / m\n",
        "\n",
        "                # Compute gradient for this parameter\n",
        "                if param_derivative is not None:\n",
        "                    grad = np.sum(output_derivative * param_derivative)\n",
        "                    gradients.append(grad)\n",
        "\n",
        "                param_count += 1\n",
        "\n",
        "        return np.array(gradients)\n",
        "\n",
        "    def train_step(self, X, Y, learning_rate):\n",
        "        # Compute gradients for all parameters\n",
        "        gradients = self.compute_gradients(X, Y)\n",
        "\n",
        "        # Update parameters in each layer\n",
        "        param_start = 0\n",
        "        for layer in self.layers:\n",
        "            param_end = param_start + layer.n_params\n",
        "            layer.update_parameters(learning_rate, gradients[param_start:param_end])\n",
        "            param_start = param_end\n",
        "\n",
        "    def compute_loss(self, predictions, Y):\n",
        "        m = Y.shape[1]\n",
        "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Perform training step\n",
        "            self.train_step(X, Y, learning_rate)\n",
        "\n",
        "            # Compute loss\n",
        "            output = self.forward(X)\n",
        "            loss = self.compute_loss(output, Y)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")"
      ],
      "metadata": {
        "id": "WIrx4bMmPJoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(num_samples=1000):\n",
        "    x = np.random.uniform(0, 1, num_samples)\n",
        "    y = np.random.uniform(0, 1, num_samples)\n",
        "    f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
        "    return np.vstack((x, y)), f_xy.reshape(1, -1)\n",
        "\n",
        "# Define the neural network\n",
        "nn = NeuralNetwork(input_size=2)\n",
        "\n",
        "# Add layers: 3 hidden layers with 10 neurons each, ReLU activation\n",
        "nn.add_layer(Layer(2, 10, activation='relu'))\n",
        "nn.add_layer(Layer(10, 10, activation='relu'))\n",
        "nn.add_layer(Layer(10, 10, activation='relu'))\n",
        "\n",
        "# Output layer with 1 neuron, no activation (linear output)\n",
        "nn.add_layer(Layer(10, 1, activation='linear'))\n",
        "\n",
        "# Training the network\n",
        "X, Y = generate_data(1000)  # Generate 1000 samples\n",
        "epochs = 5000\n",
        "learning_rate = 0.01\n",
        "\n",
        "nn.train(X, Y, epochs, learning_rate)\n",
        "\n",
        "# Test the network\n",
        "test_X, test_Y = generate_data(100)  # Generate test data\n",
        "predictions = nn.forward(test_X)\n",
        "\n",
        "# Compute test loss\n",
        "test_loss = nn.compute_loss(predictions, test_Y)\n",
        "print(f\"Test Loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCk1Wu-XThxA",
        "outputId": "d3ec7b7a-a37e-4a1d-cfad-29cf31491af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5000 complete, Loss: 0.3723659796208631\n",
            "Epoch 100/5000 complete, Loss: 0.09071779304126253\n",
            "Epoch 200/5000 complete, Loss: 0.09093226253849003\n",
            "Epoch 300/5000 complete, Loss: 0.09110324583944299\n",
            "Epoch 400/5000 complete, Loss: 0.09133402240723897\n",
            "Epoch 500/5000 complete, Loss: 0.09154483128621708\n",
            "Epoch 600/5000 complete, Loss: 0.09160357637130526\n",
            "Epoch 700/5000 complete, Loss: 0.09156012650822834\n",
            "Epoch 800/5000 complete, Loss: 0.09147719677056211\n",
            "Epoch 900/5000 complete, Loss: 0.0913229611521464\n",
            "Epoch 1000/5000 complete, Loss: 0.09113758881294831\n",
            "Epoch 1100/5000 complete, Loss: 0.09091774053314575\n",
            "Epoch 1200/5000 complete, Loss: 0.09067606068686922\n",
            "Epoch 1300/5000 complete, Loss: 0.09044892454801591\n",
            "Epoch 1400/5000 complete, Loss: 0.09038488537653026\n",
            "Epoch 1500/5000 complete, Loss: 0.09079599106760092\n",
            "Epoch 1600/5000 complete, Loss: 0.09086624793955517\n",
            "Epoch 1700/5000 complete, Loss: 0.0907590646886624\n",
            "Epoch 1800/5000 complete, Loss: 0.09062125830627724\n",
            "Epoch 1900/5000 complete, Loss: 0.09048499868227795\n",
            "Epoch 2000/5000 complete, Loss: 0.09029599252649557\n",
            "Epoch 2100/5000 complete, Loss: 0.09008185026315721\n",
            "Epoch 2200/5000 complete, Loss: 0.0898897168491555\n",
            "Epoch 2300/5000 complete, Loss: 0.0897330894050752\n",
            "Epoch 2400/5000 complete, Loss: 0.08958412714623722\n",
            "Epoch 2500/5000 complete, Loss: 0.08945585786988991\n",
            "Epoch 2600/5000 complete, Loss: 0.08934397799933047\n",
            "Epoch 2700/5000 complete, Loss: 0.08924282289906667\n",
            "Epoch 2800/5000 complete, Loss: 0.0891550982894548\n",
            "Epoch 2900/5000 complete, Loss: 0.08907440649115014\n",
            "Epoch 3000/5000 complete, Loss: 0.08899873106015561\n",
            "Epoch 3100/5000 complete, Loss: 0.08892684293878506\n",
            "Epoch 3200/5000 complete, Loss: 0.08886062607447082\n",
            "Epoch 3300/5000 complete, Loss: 0.0887974936949558\n",
            "Epoch 3400/5000 complete, Loss: 0.08874962723471626\n",
            "Epoch 3500/5000 complete, Loss: 0.08870827733214395\n",
            "Epoch 3600/5000 complete, Loss: 0.08867188459118737\n",
            "Epoch 3700/5000 complete, Loss: 0.08863836298918393\n",
            "Epoch 3800/5000 complete, Loss: 0.08860606602574372\n",
            "Epoch 3900/5000 complete, Loss: 0.08857572843386627\n",
            "Epoch 4000/5000 complete, Loss: 0.08854858472250328\n",
            "Epoch 4100/5000 complete, Loss: 0.08852193228305465\n",
            "Epoch 4200/5000 complete, Loss: 0.08849429428686192\n",
            "Epoch 4300/5000 complete, Loss: 0.08846823252476574\n",
            "Epoch 4400/5000 complete, Loss: 0.088449467365767\n",
            "Epoch 4500/5000 complete, Loss: 0.08843385853193181\n",
            "Epoch 4600/5000 complete, Loss: 0.08842212035143485\n",
            "Epoch 4700/5000 complete, Loss: 0.08841273434581053\n",
            "Epoch 4800/5000 complete, Loss: 0.0884060317365878\n",
            "Epoch 4900/5000 complete, Loss: 0.08840148204871173\n",
            "Test Loss: 0.09140004118774286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4P9vlj9eP-F-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}