{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "RkLSfB-XKFG3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "zOrkgVQrKLmN"
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        else:\n",
    "            return lambda x: x\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.z = np.dot(self.weights, input_data) + self.biases\n",
    "        self.a = self.activation_function(self.z)\n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "IngBVpXoKU5V"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_data = X\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3eMtR1fKaoT",
    "outputId": "5e31d841-449b-45d0-c703-b9026cfcb9c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29576971]]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(input_size=2)\n",
    "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
    "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
    "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Third hidden\n",
    "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
    "\n",
    "x = np.random.rand()\n",
    "y = np.random.rand()\n",
    "\n",
    "input_vector = np.array([x, y]).reshape(2, 1)\n",
    "output_vector = nn.forward(input_vector)\n",
    "print(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "M2syVSfK13db"
   },
   "outputs": [],
   "source": [
    "# reverse mode AD\n",
    "\n",
    "class ReverseLayer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, (output_size, input_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "        # Store activation function\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        else:\n",
    "            return lambda x: x  # Linear activation\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.z = np.dot(self.weights, input_data) + self.biases\n",
    "        self.a = self.activation_function(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, dA):\n",
    "        m = self.input_data.shape[1]  # Batch size\n",
    "        self.dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
    "        self.dW = np.dot(self.dZ, self.input_data.T) / m\n",
    "        self.db = np.sum(self.dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(self.weights.T, self.dZ)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "class ReverseNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_data = X\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def compute_gradients(self, Y, learning_rate):\n",
    "        m = Y.shape[1]\n",
    "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SnAlh1xW2A9E",
    "outputId": "96001d25-b427-4aad-f11f-afbc022038a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10, 1)\n",
      "(10, 10) (10, 1)\n",
      "(10, 10) (10, 1)\n",
      "(1, 10) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "nn = ReverseNeuralNetwork()\n",
    "\n",
    "layer1 = ReverseLayer(input_size=2, output_size=10, activation='relu')\n",
    "layer2 = ReverseLayer(input_size=10, output_size=10, activation='relu')\n",
    "layer3 = ReverseLayer(input_size=10, output_size=10, activation='relu')\n",
    "output_layer = ReverseLayer(input_size=10, output_size=1, activation='linear')\n",
    "\n",
    "nn.add_layer(layer1)  # First hidden layer\n",
    "nn.add_layer(layer2)  # Second hidden layer\n",
    "nn.add_layer(layer3)  # Third hidden layer\n",
    "nn.add_layer(output_layer)  # Output layer\n",
    "\n",
    "x = np.random.rand()\n",
    "y = np.random.rand()\n",
    "f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
    "\n",
    "input_vector = np.array([x, y]).reshape(2, 1)\n",
    "true_output = np.array([[f_xy]])\n",
    "output_vector = nn.forward(input_vector)\n",
    "nn.compute_gradients(true_output, learning_rate=0.01)\n",
    "\n",
    "print(layer1.dW.shape, layer1.db.shape)\n",
    "print(layer2.dW.shape, layer2.db.shape)\n",
    "print(layer3.dW.shape, layer3.db.shape)\n",
    "print(output_layer.dW.shape, output_layer.db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EfX5H0XEH41",
    "outputId": "762d1bb2-cc97-4bbe-86f1-378d99a16f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time:0.00014938497543334962\n"
     ]
    }
   ],
   "source": [
    "# average time to compute gradients\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "\n",
    "  nn = ReverseNeuralNetwork()\n",
    "\n",
    "  layer1 = ReverseLayer(input_size=2, output_size=10, activation='relu')\n",
    "  layer2 = ReverseLayer(input_size=10, output_size=10, activation='relu')\n",
    "  layer3 = ReverseLayer(input_size=10, output_size=10, activation='relu')\n",
    "  output_layer = ReverseLayer(input_size=10, output_size=1, activation='linear')\n",
    "\n",
    "  nn.add_layer(layer1)  # First hidden layer\n",
    "  nn.add_layer(layer2)  # Second hidden layer\n",
    "  nn.add_layer(layer3)  # Third hidden layer\n",
    "  nn.add_layer(output_layer)  # Output layer\n",
    "\n",
    "  x = np.random.rand()\n",
    "  y = np.random.rand()\n",
    "\n",
    "  input_vector = np.array([x, y]).reshape(2, 1)\n",
    "  output_vector = nn.forward(input_vector)\n",
    "  nn.compute_gradients(true_output, learning_rate=0.01)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Average Time:\" + str((end_time - start_time) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "gGvusmBJ3LmK"
   },
   "outputs": [],
   "source": [
    "# forward mode ad\n",
    "\n",
    "class ForwardLayer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, (output_size, input_size))\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "        self.n_params = output_size * input_size + output_size\n",
    "\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        else:\n",
    "            return lambda x: x\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return lambda x: np.ones_like(x)\n",
    "\n",
    "    def forward(self, input_data, param_index=None):\n",
    "        self.input_data = input_data\n",
    "        batch_size = input_data.shape[1]\n",
    "\n",
    "        # Initialize derivatives\n",
    "        w_size = self.output_size * self.input_size\n",
    "        if param_index is not None:\n",
    "            # Create derivative matrices\n",
    "            dw = np.zeros_like(self.weights)\n",
    "            db = np.zeros_like(self.biases)\n",
    "\n",
    "            if param_index < w_size:\n",
    "                # This is a weight parameter\n",
    "                i, j = param_index // self.input_size, param_index % self.input_size\n",
    "                dw[i, j] = 1.0\n",
    "            else:\n",
    "                # This is a bias parameter\n",
    "                bias_idx = param_index - w_size\n",
    "                db[bias_idx] = 1.0\n",
    "\n",
    "        # Forward pass\n",
    "        z = np.dot(self.weights, input_data) + self.biases\n",
    "        a = self.activation_function(z)\n",
    "\n",
    "        if param_index is not None:\n",
    "            # Compute derivative of z\n",
    "            dz = np.dot(dw, input_data) + db\n",
    "            # Compute derivative of activation\n",
    "            da = self.activation_derivative(z) * dz\n",
    "            return a, da\n",
    "        return a, None\n",
    "\n",
    "    def update_parameters(self, learning_rate, gradients):\n",
    "        w_size = self.output_size * self.input_size\n",
    "\n",
    "        # Reshape gradients for weights and biases\n",
    "        dW = gradients[:w_size].reshape(self.output_size, self.input_size)\n",
    "        db = gradients[w_size:].reshape(self.output_size, 1)\n",
    "\n",
    "        # Update parameters\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "class ForwardNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_data = X\n",
    "        for layer in self.layers:\n",
    "            input_data, _ = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def get_total_params(self):\n",
    "        return sum(layer.n_params for layer in self.layers)\n",
    "\n",
    "    def compute_gradients(self, X, Y):\n",
    "        n_total_params = self.get_total_params()\n",
    "        gradients = []\n",
    "\n",
    "        # For each parameter in the network\n",
    "        param_count = 0\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            for param_idx in range(layer.n_params):\n",
    "                # Forward pass with derivative with respect to this parameter\n",
    "                input_data = X\n",
    "                param_derivative = None\n",
    "\n",
    "                for i, current_layer in enumerate(self.layers):\n",
    "                    if i == layer_idx:\n",
    "                        input_data, param_derivative = current_layer.forward(input_data, param_idx)\n",
    "                    else:\n",
    "                        input_data, _ = current_layer.forward(input_data)\n",
    "\n",
    "                # Compute loss derivative\n",
    "                m = Y.shape[1]\n",
    "                output_derivative = (input_data - Y) / m\n",
    "\n",
    "                # Compute gradient for this parameter\n",
    "                if param_derivative is not None:\n",
    "                    grad = np.sum(output_derivative * param_derivative)\n",
    "                    gradients.append(grad)\n",
    "\n",
    "                param_count += 1\n",
    "\n",
    "        return np.array(gradients)\n",
    "\n",
    "    def get_layer_gradients(self, layer_index, gradients):\n",
    "\n",
    "      # Find the starting index for this layer's parameters\n",
    "      start_idx = 0\n",
    "      for i in range(layer_index):\n",
    "          start_idx += self.layers[i].n_params\n",
    "\n",
    "      # Get the layer\n",
    "      layer = self.layers[layer_index]\n",
    "\n",
    "      # Extract weight gradients\n",
    "      w_size = layer.output_size * layer.input_size\n",
    "      dW = gradients[start_idx:start_idx + w_size].reshape(layer.output_size, layer.input_size)\n",
    "\n",
    "      # Extract bias gradients\n",
    "      db = gradients[start_idx + w_size:start_idx + layer.n_params].reshape(layer.output_size, 1)\n",
    "\n",
    "      return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TYDpyvI4bq0",
    "outputId": "8e1054ab-2fa8-4c14-b7d4-6c5a9d1a6772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10, 1)\n",
      "(10, 10) (10, 1)\n",
      "(10, 10) (10, 1)\n",
      "(1, 10) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "nn = ForwardNeuralNetwork()\n",
    "\n",
    "layer1 = ForwardLayer(input_size=2, output_size=10, activation='relu')\n",
    "layer2 = ForwardLayer(input_size=10, output_size=10, activation='relu')\n",
    "layer3 = ForwardLayer(input_size=10, output_size=10, activation='relu')\n",
    "output_layer = ForwardLayer(input_size=10, output_size=1, activation='linear')\n",
    "\n",
    "nn.add_layer(layer1)  # First hidden layer\n",
    "nn.add_layer(layer2)  # Second hidden layer\n",
    "nn.add_layer(layer3)  # Third hidden layer\n",
    "nn.add_layer(output_layer)  # Output layer\n",
    "\n",
    "x = np.random.rand()\n",
    "y = np.random.rand()\n",
    "f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
    "\n",
    "input_vector = np.array([x, y]).reshape(2, 1)\n",
    "true_output = np.array([[f_xy]])\n",
    "\n",
    "output_vector = nn.forward(input_vector)\n",
    "gradients = nn.compute_gradients(input_vector, true_output)\n",
    "\n",
    "dW0, db0 = nn.get_layer_gradients(layer_index=0, gradients=gradients)\n",
    "print(dW0.shape, db0.shape)\n",
    "dW1, db1 = nn.get_layer_gradients(layer_index=1, gradients=gradients)\n",
    "print(dW1.shape, db1.shape)\n",
    "dW2, db2 = nn.get_layer_gradients(layer_index=2, gradients=gradients)\n",
    "print(dW2.shape, db2.shape)\n",
    "dW3, db3 = nn.get_layer_gradients(layer_index=3, gradients=gradients)\n",
    "print(dW3.shape, db3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ub74aT2BgH1",
    "outputId": "00eb6363-a320-479c-c1bf-5237e2b230e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time:0.013175784349441529\n"
     ]
    }
   ],
   "source": [
    "# average time to compute gradients\n",
    "start_time = time.time()\n",
    "for i in range(1000):\n",
    "  nn = ForwardNeuralNetwork()\n",
    "\n",
    "  layer1 = ForwardLayer(input_size=2, output_size=10, activation='relu')\n",
    "  layer2 = ForwardLayer(input_size=10, output_size=10, activation='relu')\n",
    "  layer3 = ForwardLayer(input_size=10, output_size=10, activation='relu')\n",
    "  output_layer = ForwardLayer(input_size=10, output_size=1, activation='linear')\n",
    "\n",
    "  nn.add_layer(layer1)  # First hidden layer\n",
    "  nn.add_layer(layer2)  # Second hidden layer\n",
    "  nn.add_layer(layer3)  # Third hidden layer\n",
    "  nn.add_layer(output_layer)  # Output layer\n",
    "\n",
    "  x = np.random.rand()\n",
    "  y = np.random.rand()\n",
    "\n",
    "  input_vector = np.array([x, y]).reshape(2, 1)\n",
    "  output_vector = nn.forward(input_vector)\n",
    "  gradients = nn.compute_gradients(input_vector, output_vector)\n",
    "end_time = time.time()\n",
    "print(\"Average Time:\" + str((end_time - start_time) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "orAyCEg7Fl17"
   },
   "outputs": [],
   "source": [
    "# reverse mode training\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "        # Store activation function\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        else:\n",
    "            return lambda x: x  # Linear activation\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.z = np.dot(self.weights, input_data) + self.biases\n",
    "        self.a = self.activation_function(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, dA, learning_rate):\n",
    "        m = self.input_data.shape[1]  # Batch size\n",
    "        dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
    "        dW = np.dot(dZ, self.input_data.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(self.weights.T, dZ)\n",
    "\n",
    "        # Update weights and biases using SGD\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_data = X\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def backward(self, Y, learning_rate):\n",
    "        m = Y.shape[1]\n",
    "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA, learning_rate)\n",
    "\n",
    "    def compute_loss(self, predictions, Y):\n",
    "        m = Y.shape[1]\n",
    "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(output, Y)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(Y, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_bc0vb8pFhBj",
    "outputId": "0a2a9ac5-351c-40c9-977e-12ae870abcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000 complete, Loss: 1.685471974215143\n",
      "Epoch 100/5000 complete, Loss: 0.07531531499099711\n",
      "Epoch 200/5000 complete, Loss: 0.07439336372513458\n",
      "Epoch 300/5000 complete, Loss: 0.07371612809878492\n",
      "Epoch 400/5000 complete, Loss: 0.07314506998362563\n",
      "Epoch 500/5000 complete, Loss: 0.07262280225583413\n",
      "Epoch 600/5000 complete, Loss: 0.07212022411747808\n",
      "Epoch 700/5000 complete, Loss: 0.0716345058138881\n",
      "Epoch 800/5000 complete, Loss: 0.0711500901956785\n",
      "Epoch 900/5000 complete, Loss: 0.07065672160380214\n",
      "Epoch 1000/5000 complete, Loss: 0.07014824145622603\n",
      "Epoch 1100/5000 complete, Loss: 0.06962557946022217\n",
      "Epoch 1200/5000 complete, Loss: 0.0690888906705081\n",
      "Epoch 1300/5000 complete, Loss: 0.06851064239113618\n",
      "Epoch 1400/5000 complete, Loss: 0.06788335109688907\n",
      "Epoch 1500/5000 complete, Loss: 0.06720790050104128\n",
      "Epoch 1600/5000 complete, Loss: 0.06647056489964402\n",
      "Epoch 1700/5000 complete, Loss: 0.06565448269228022\n",
      "Epoch 1800/5000 complete, Loss: 0.06476178238990161\n",
      "Epoch 1900/5000 complete, Loss: 0.06372385232042828\n",
      "Epoch 2000/5000 complete, Loss: 0.06255457655696599\n",
      "Epoch 2100/5000 complete, Loss: 0.06102807584009499\n",
      "Epoch 2200/5000 complete, Loss: 0.05906261984033219\n",
      "Epoch 2300/5000 complete, Loss: 0.056584563736545454\n",
      "Epoch 2400/5000 complete, Loss: 0.053519301751946115\n",
      "Epoch 2500/5000 complete, Loss: 0.051037804078372516\n",
      "Epoch 2600/5000 complete, Loss: 0.04887741237073481\n",
      "Epoch 2700/5000 complete, Loss: 0.04682546313782127\n",
      "Epoch 2800/5000 complete, Loss: 0.04480913156674053\n",
      "Epoch 2900/5000 complete, Loss: 0.04276391835848473\n",
      "Epoch 3000/5000 complete, Loss: 0.04071896482144053\n",
      "Epoch 3100/5000 complete, Loss: 0.03873489313554032\n",
      "Epoch 3200/5000 complete, Loss: 0.03679327527588813\n",
      "Epoch 3300/5000 complete, Loss: 0.03487828635670529\n",
      "Epoch 3400/5000 complete, Loss: 0.032973336583164536\n",
      "Epoch 3500/5000 complete, Loss: 0.031163023059650137\n",
      "Epoch 3600/5000 complete, Loss: 0.029509344500499492\n",
      "Epoch 3700/5000 complete, Loss: 0.027995398842654652\n",
      "Epoch 3800/5000 complete, Loss: 0.026632279456791436\n",
      "Epoch 3900/5000 complete, Loss: 0.025410332209687095\n",
      "Epoch 4000/5000 complete, Loss: 0.024280088878958574\n",
      "Epoch 4100/5000 complete, Loss: 0.02326626070310448\n",
      "Epoch 4200/5000 complete, Loss: 0.022376067112540313\n",
      "Epoch 4300/5000 complete, Loss: 0.021591452983389506\n",
      "Epoch 4400/5000 complete, Loss: 0.020915497018048207\n",
      "Epoch 4500/5000 complete, Loss: 0.020324777660585987\n",
      "Epoch 4600/5000 complete, Loss: 0.01980190501344513\n",
      "Epoch 4700/5000 complete, Loss: 0.019327375302990786\n",
      "Epoch 4800/5000 complete, Loss: 0.01890346394210416\n",
      "Epoch 4900/5000 complete, Loss: 0.018518577737023284\n",
      "Time:4.6085145473480225\n",
      "Test Loss: 0.012305798511963662\n"
     ]
    }
   ],
   "source": [
    "def generate_data(num_samples=1000):\n",
    "    x = np.random.uniform(0, 1, num_samples)\n",
    "    y = np.random.uniform(0, 1, num_samples)\n",
    "    f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
    "    return np.vstack((x, y)), f_xy.reshape(1, -1)\n",
    "\n",
    "# Define the neural network\n",
    "nn = NeuralNetwork(input_size=2)\n",
    "\n",
    "# Add layers: 3 hidden layers with 10 neurons each, ReLU activation\n",
    "nn.add_layer(Layer(2, 10, activation='relu'))\n",
    "nn.add_layer(Layer(10, 10, activation='relu'))\n",
    "nn.add_layer(Layer(10, 10, activation='relu'))\n",
    "\n",
    "# Output layer with 1 neuron, no activation (linear output)\n",
    "nn.add_layer(Layer(10, 1, activation='linear'))\n",
    "\n",
    "# Training the network\n",
    "X, Y = generate_data(1000)  # Generate 1000 samples\n",
    "epochs = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "nn.train(X, Y, epochs, learning_rate)\n",
    "end_time = time.time()\n",
    "print(\"Time:\" + str((end_time - start_time)))\n",
    "\n",
    "# Test the network\n",
    "test_X, test_Y = generate_data(100)  # Generate test data\n",
    "predictions = nn.forward(test_X)\n",
    "\n",
    "# Compute test loss\n",
    "test_loss = nn.compute_loss(predictions, test_Y)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "RRk7lUsmFvuZ"
   },
   "outputs": [],
   "source": [
    "# forward mode training\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "\n",
    "        # Total number of parameters\n",
    "        self.n_params = output_size * input_size + output_size\n",
    "\n",
    "        # Store activation function\n",
    "        self.activation_function = self.get_activation_function(activation)\n",
    "        self.activation_derivative = self.get_activation_derivative(activation)\n",
    "\n",
    "    def get_activation_function(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: np.maximum(0, x)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: np.tanh(x)\n",
    "        else:\n",
    "            return lambda x: x\n",
    "\n",
    "    def get_activation_derivative(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: (x > 0).astype(float)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
    "        elif activation == 'tanh':\n",
    "            return lambda x: 1 - np.tanh(x)**2\n",
    "        else:\n",
    "            return lambda x: np.ones_like(x)\n",
    "\n",
    "    def forward(self, input_data, param_index=None):\n",
    "        self.input_data = input_data\n",
    "        batch_size = input_data.shape[1]\n",
    "\n",
    "        # Initialize derivatives\n",
    "        w_size = self.output_size * self.input_size\n",
    "        if param_index is not None:\n",
    "            # Create derivative matrices\n",
    "            dw = np.zeros_like(self.weights)\n",
    "            db = np.zeros_like(self.biases)\n",
    "\n",
    "            if param_index < w_size:\n",
    "                # This is a weight parameter\n",
    "                i, j = param_index // self.input_size, param_index % self.input_size\n",
    "                dw[i, j] = 1.0\n",
    "            else:\n",
    "                # This is a bias parameter\n",
    "                bias_idx = param_index - w_size\n",
    "                db[bias_idx] = 1.0\n",
    "\n",
    "        # Forward pass\n",
    "        z = np.dot(self.weights, input_data) + self.biases\n",
    "        a = self.activation_function(z)\n",
    "\n",
    "        if param_index is not None:\n",
    "            # Compute derivative of z\n",
    "            dz = np.dot(dw, input_data) + db\n",
    "            # Compute derivative of activation\n",
    "            da = self.activation_derivative(z) * dz\n",
    "            return a, da\n",
    "        return a, None\n",
    "\n",
    "    def update_parameters(self, learning_rate, gradients):\n",
    "        w_size = self.output_size * self.input_size\n",
    "\n",
    "        # Reshape gradients for weights and biases\n",
    "        dW = gradients[:w_size].reshape(self.output_size, self.input_size)\n",
    "        db = gradients[w_size:].reshape(self.output_size, 1)\n",
    "\n",
    "        # Update parameters\n",
    "        self.weights -= learning_rate * dW\n",
    "        self.biases -= learning_rate * db\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_data = X\n",
    "        for layer in self.layers:\n",
    "            input_data, _ = layer.forward(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def get_total_params(self):\n",
    "        return sum(layer.n_params for layer in self.layers)\n",
    "\n",
    "    def compute_gradients(self, X, Y):\n",
    "        n_total_params = self.get_total_params()\n",
    "        gradients = []\n",
    "\n",
    "        # For each parameter in the network\n",
    "        param_count = 0\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            for param_idx in range(layer.n_params):\n",
    "                # Forward pass with derivative with respect to this parameter\n",
    "                input_data = X\n",
    "                param_derivative = None\n",
    "\n",
    "                for i, current_layer in enumerate(self.layers):\n",
    "                    if i == layer_idx:\n",
    "                        input_data, param_derivative = current_layer.forward(input_data, param_idx)\n",
    "                    else:\n",
    "                        input_data, _ = current_layer.forward(input_data)\n",
    "\n",
    "                # Compute loss derivative\n",
    "                m = Y.shape[1]\n",
    "                output_derivative = (input_data - Y) / m\n",
    "\n",
    "                # Compute gradient for this parameter\n",
    "                if param_derivative is not None:\n",
    "                    grad = np.sum(output_derivative * param_derivative)\n",
    "                    gradients.append(grad)\n",
    "\n",
    "                param_count += 1\n",
    "\n",
    "        return np.array(gradients)\n",
    "\n",
    "    def train_step(self, X, Y, learning_rate):\n",
    "        # Compute gradients for all parameters\n",
    "        gradients = self.compute_gradients(X, Y)\n",
    "\n",
    "        # Update parameters in each layer\n",
    "        param_start = 0\n",
    "        for layer in self.layers:\n",
    "            param_end = param_start + layer.n_params\n",
    "            layer.update_parameters(learning_rate, gradients[param_start:param_end])\n",
    "            param_start = param_end\n",
    "\n",
    "    def compute_loss(self, predictions, Y):\n",
    "        m = Y.shape[1]\n",
    "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
    "\n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            # Perform training step\n",
    "            self.train_step(X, Y, learning_rate)\n",
    "\n",
    "            # Compute loss\n",
    "            output = self.forward(X)\n",
    "            loss = self.compute_loss(output, Y)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg4OYd98GknY",
    "outputId": "11a56d05-d06a-462a-bfba-3a7eda1a1388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000 complete, Loss: 0.4518593406041179\n",
      "Epoch 100/5000 complete, Loss: 0.09879157851573364\n",
      "Epoch 200/5000 complete, Loss: 0.09762491947873883\n",
      "Epoch 300/5000 complete, Loss: 0.09404424000222868\n",
      "Epoch 400/5000 complete, Loss: 0.09052185802619502\n",
      "Epoch 500/5000 complete, Loss: 0.08773187798010927\n",
      "Epoch 600/5000 complete, Loss: 0.08571110119965288\n",
      "Epoch 700/5000 complete, Loss: 0.08409964832784013\n",
      "Epoch 800/5000 complete, Loss: 0.08275943694737176\n",
      "Epoch 900/5000 complete, Loss: 0.08169743990258306\n",
      "Epoch 1000/5000 complete, Loss: 0.0808333286927836\n",
      "Epoch 1100/5000 complete, Loss: 0.0801113235893606\n",
      "Epoch 1200/5000 complete, Loss: 0.07949472751755221\n",
      "Epoch 1300/5000 complete, Loss: 0.0789742265329628\n",
      "Epoch 1400/5000 complete, Loss: 0.07854155803984031\n",
      "Epoch 1500/5000 complete, Loss: 0.07819327103486656\n",
      "Epoch 1600/5000 complete, Loss: 0.07794236493746283\n",
      "Epoch 1700/5000 complete, Loss: 0.07770301012833228\n",
      "Epoch 1800/5000 complete, Loss: 0.07748005448258957\n",
      "Epoch 1900/5000 complete, Loss: 0.07731785614164759\n",
      "Epoch 2000/5000 complete, Loss: 0.07715878606625559\n",
      "Epoch 2100/5000 complete, Loss: 0.07695701102259844\n",
      "Epoch 2200/5000 complete, Loss: 0.07682281759346012\n",
      "Epoch 2300/5000 complete, Loss: 0.07672555823665225\n",
      "Epoch 2400/5000 complete, Loss: 0.07664482796464427\n",
      "Epoch 2500/5000 complete, Loss: 0.07657338629777946\n",
      "Epoch 2600/5000 complete, Loss: 0.07650860115821012\n",
      "Epoch 2700/5000 complete, Loss: 0.07644502832835215\n",
      "Epoch 2800/5000 complete, Loss: 0.07640016623224025\n",
      "Epoch 2900/5000 complete, Loss: 0.07636013322411597\n",
      "Epoch 3000/5000 complete, Loss: 0.07631825243017494\n",
      "Epoch 3100/5000 complete, Loss: 0.07629042110548852\n",
      "Epoch 3200/5000 complete, Loss: 0.07626320141857278\n",
      "Epoch 3300/5000 complete, Loss: 0.0762418094525524\n",
      "Epoch 3400/5000 complete, Loss: 0.07623970160027153\n",
      "Epoch 3500/5000 complete, Loss: 0.07624030291521645\n",
      "Epoch 3600/5000 complete, Loss: 0.07624045247190239\n",
      "Epoch 3700/5000 complete, Loss: 0.07623795639558986\n",
      "Epoch 3800/5000 complete, Loss: 0.07623554007533903\n",
      "Epoch 3900/5000 complete, Loss: 0.07623852438870221\n",
      "Epoch 4000/5000 complete, Loss: 0.07623987192314635\n",
      "Epoch 4100/5000 complete, Loss: 0.07624169052338842\n",
      "Epoch 4200/5000 complete, Loss: 0.07624241330813905\n",
      "Epoch 4300/5000 complete, Loss: 0.0762424613448875\n",
      "Epoch 4400/5000 complete, Loss: 0.0762434876669208\n",
      "Epoch 4500/5000 complete, Loss: 0.07624860616544862\n",
      "Epoch 4600/5000 complete, Loss: 0.07625350415623376\n",
      "Epoch 4700/5000 complete, Loss: 0.07625893667242167\n",
      "Epoch 4800/5000 complete, Loss: 0.07626412022693808\n",
      "Epoch 4900/5000 complete, Loss: 0.0762685927250866\n",
      "Time:637.7847752571106\n",
      "Test Loss: 0.0781395239659479\n"
     ]
    }
   ],
   "source": [
    "def generate_data(num_samples=1000):\n",
    "    x = np.random.uniform(0, 1, num_samples)\n",
    "    y = np.random.uniform(0, 1, num_samples)\n",
    "    f_xy = np.sin(2 * np.pi * x * y) + 2 * x * y**2\n",
    "    return np.vstack((x, y)), f_xy.reshape(1, -1)\n",
    "\n",
    "# Define the neural network\n",
    "nn = NeuralNetwork(input_size=2)\n",
    "\n",
    "# Add layers: 3 hidden layers with 10 neurons each, ReLU activation\n",
    "nn.add_layer(Layer(2, 10, activation='relu'))\n",
    "nn.add_layer(Layer(10, 10, activation='relu'))\n",
    "nn.add_layer(Layer(10, 10, activation='relu'))\n",
    "\n",
    "# Output layer with 1 neuron, no activation (linear output)\n",
    "nn.add_layer(Layer(10, 1, activation='linear'))\n",
    "\n",
    "# Training the network\n",
    "X, Y = generate_data(1000)  # Generate 1000 samples\n",
    "epochs = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "nn.train(X, Y, epochs, learning_rate)\n",
    "end_time = time.time()\n",
    "print(\"Time:\" + str((end_time - start_time)))\n",
    "\n",
    "# Test the network\n",
    "test_X, test_Y = generate_data(100)  # Generate test data\n",
    "predictions = nn.forward(test_X)\n",
    "\n",
    "# Compute test loss\n",
    "test_loss = nn.compute_loss(predictions, test_Y)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
