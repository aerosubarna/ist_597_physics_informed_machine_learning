{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eakj9vDZ_MD3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x  # Linear activation\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        self.a = self.activation_function(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, dA, learning_rate):\n",
        "        m = self.input_data.shape[1]  # Batch size\n",
        "        dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
        "        dW = np.dot(dZ, self.input_data.T) / m\n",
        "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(self.weights.T, dZ)\n",
        "\n",
        "        # Update weights and biases using SGD\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.biases -= learning_rate * db\n",
        "\n",
        "        return dA_prev"
      ],
      "metadata": {
        "id": "-p4vh9Z6_Wa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 3\n",
        "output_size = 2\n",
        "layer = Layer(input_size, output_size, activation='sigmoid')\n",
        "\n",
        "input_vector = np.array([1, 2, 3]).reshape(input_size, 1)\n",
        "output_vector = layer.forward(input_vector)\n",
        "print(output_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEzsuuRx_ZGR",
        "outputId": "1945e969-5b23-490a-d6fb-0446e1b1b71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.7652793 ]\n",
            " [0.56028969]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def backward(self, Y, learning_rate):\n",
        "        m = Y.shape[1]\n",
        "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA, learning_rate)\n",
        "\n",
        "    def compute_loss(self, predictions, Y):\n",
        "        m = Y.shape[1]\n",
        "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            output = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(output, Y)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(Y, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "DZPTehFD_bPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "x = np.random.rand()\n",
        "y = np.random.rand()\n",
        "\n",
        "input_vector = np.array([x, y]).reshape(2, 1)\n",
        "output_vector = nn.forward(input_vector)\n",
        "print(output_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAu14hdJb1Fu",
        "outputId": "c0384a34-f0de-44af-f537-383a24f99103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.37613905]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input-output function y = sin(2πxy) + 2xy²\n",
        "def target_function(x, y):\n",
        "    return np.sin(2 * np.pi * x * y) + 2 * x * y ** 2\n",
        "\n",
        "# Generate random training data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(2, 1000)  # 1000 samples, x and y between 0 and 1\n",
        "Y = target_function(X[0], X[1]).reshape(1, 1000)  # Target output\n",
        "\n",
        "X_train = X[:, :800]\n",
        "Y_train = Y[:, :800]\n",
        "\n",
        "X_test = X[:, 800:]\n",
        "Y_test = Y[:, 800:]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4LO_WzVbgCR",
        "outputId": "e32b047c-4047-4efa-c23c-07d8ee82f0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 800)\n",
            "(1, 800)\n",
            "(2, 200)\n",
            "(1, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "# Train the network\n",
        "nn.train(X, Y, epochs=1000, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slnBy7XX_gtn",
        "outputId": "9988c65e-421c-4547-cd90-a804831cb63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 complete, Loss: 0.19598861028521372\n",
            "Epoch 100/1000 complete, Loss: 0.1254776894918218\n",
            "Epoch 200/1000 complete, Loss: 0.10598490966523468\n",
            "Epoch 300/1000 complete, Loss: 0.09682446242006398\n",
            "Epoch 400/1000 complete, Loss: 0.09122907623764602\n",
            "Epoch 500/1000 complete, Loss: 0.08725785805291585\n",
            "Epoch 600/1000 complete, Loss: 0.08408067261336181\n",
            "Epoch 700/1000 complete, Loss: 0.08133948207692242\n",
            "Epoch 800/1000 complete, Loss: 0.07899308889905757\n",
            "Epoch 900/1000 complete, Loss: 0.07698303896375883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the trained neural network\n",
        "Y_pred = nn.forward(X_test)\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) between true values and predicted values\n",
        "mse = np.mean((Y_pred - Y_test) ** 2)\n",
        "print(f\"Mean Squared Error on Test Data: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbdr6hr3XfiB",
        "outputId": "69977f3c-2106-4f9d-d12f-70e80aa23f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Data: 0.1582241322293819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Total number of parameters\n",
        "        self.n_params = output_size * input_size + output_size\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)\n",
        "\n",
        "    def forward(self, input_data, param_index=None):\n",
        "        self.input_data = input_data\n",
        "        batch_size = input_data.shape[1]\n",
        "\n",
        "        # Initialize derivatives\n",
        "        w_size = self.output_size * self.input_size\n",
        "        if param_index is not None:\n",
        "            # Create derivative matrices\n",
        "            dw = np.zeros_like(self.weights)\n",
        "            db = np.zeros_like(self.biases)\n",
        "\n",
        "            if param_index < w_size:\n",
        "                # This is a weight parameter\n",
        "                i, j = param_index // self.input_size, param_index % self.input_size\n",
        "                dw[i, j] = 1.0\n",
        "            else:\n",
        "                # This is a bias parameter\n",
        "                bias_idx = param_index - w_size\n",
        "                db[bias_idx] = 1.0\n",
        "\n",
        "        # Forward pass\n",
        "        z = np.dot(self.weights, input_data) + self.biases\n",
        "        a = self.activation_function(z)\n",
        "\n",
        "        if param_index is not None:\n",
        "            # Compute derivative of z\n",
        "            dz = np.dot(dw, input_data) + db\n",
        "            # Compute derivative of activation\n",
        "            da = self.activation_derivative(z) * dz\n",
        "            return a, da\n",
        "        return a, None\n",
        "\n",
        "    def update_parameters(self, learning_rate, gradients):\n",
        "        w_size = self.output_size * self.input_size\n",
        "\n",
        "        # Reshape gradients for weights and biases\n",
        "        dW = gradients[:w_size].reshape(self.output_size, self.input_size)\n",
        "        db = gradients[w_size:].reshape(self.output_size, 1)\n",
        "\n",
        "        # Update parameters\n",
        "        self.weights -= learning_rate * dW\n",
        "        self.biases -= learning_rate * db"
      ],
      "metadata": {
        "id": "viJ_Jv1tcvUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data, _ = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def get_total_params(self):\n",
        "        return sum(layer.n_params for layer in self.layers)\n",
        "\n",
        "    def compute_gradients(self, X, Y):\n",
        "        n_total_params = self.get_total_params()\n",
        "        gradients = []\n",
        "\n",
        "        # For each parameter in the network\n",
        "        param_count = 0\n",
        "        for layer_idx, layer in enumerate(self.layers):\n",
        "            for param_idx in range(layer.n_params):\n",
        "                # Forward pass with derivative with respect to this parameter\n",
        "                input_data = X\n",
        "                param_derivative = None\n",
        "\n",
        "                for i, current_layer in enumerate(self.layers):\n",
        "                    if i == layer_idx:\n",
        "                        input_data, param_derivative = current_layer.forward(input_data, param_idx)\n",
        "                    else:\n",
        "                        input_data, _ = current_layer.forward(input_data)\n",
        "\n",
        "                # Compute loss derivative\n",
        "                m = Y.shape[1]\n",
        "                output_derivative = (input_data - Y) / m\n",
        "\n",
        "                # Compute gradient for this parameter\n",
        "                if param_derivative is not None:\n",
        "                    grad = np.sum(output_derivative * param_derivative)\n",
        "                    gradients.append(grad)\n",
        "\n",
        "                param_count += 1\n",
        "\n",
        "        return np.array(gradients)\n",
        "\n",
        "    def train_step(self, X, Y, learning_rate):\n",
        "        # Compute gradients for all parameters\n",
        "        gradients = self.compute_gradients(X, Y)\n",
        "\n",
        "        # Update parameters in each layer\n",
        "        param_start = 0\n",
        "        for layer in self.layers:\n",
        "            param_end = param_start + layer.n_params\n",
        "            layer.update_parameters(learning_rate, gradients[param_start:param_end])\n",
        "            param_start = param_end\n",
        "\n",
        "    def compute_loss(self, predictions, Y):\n",
        "        m = Y.shape[1]\n",
        "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            # Perform training step\n",
        "            self.train_step(X, Y, learning_rate)\n",
        "\n",
        "            # Compute loss\n",
        "            output = self.forward(X)\n",
        "            loss = self.compute_loss(output, Y)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs} complete, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "AhTiSV9HfYyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize neural network\n",
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "# Train the network\n",
        "nn.train(X, Y, epochs=1000, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbQbXGKimtMY",
        "outputId": "f30b4df0-c4b4-4e0d-d97d-22de7c13e355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000 complete, Loss: 0.4125869786199614\n",
            "Epoch 100/1000 complete, Loss: 0.08828936666874818\n",
            "Epoch 200/1000 complete, Loss: 0.08416290359280922\n",
            "Epoch 300/1000 complete, Loss: 0.08188915488313034\n",
            "Epoch 400/1000 complete, Loss: 0.08062927155411227\n",
            "Epoch 500/1000 complete, Loss: 0.07989338381246565\n",
            "Epoch 600/1000 complete, Loss: 0.07943426453093261\n",
            "Epoch 700/1000 complete, Loss: 0.079135729132181\n",
            "Epoch 800/1000 complete, Loss: 0.07892120120191445\n",
            "Epoch 900/1000 complete, Loss: 0.0787612458397065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data using the trained neural network\n",
        "Y_pred = nn.forward(X_test)\n",
        "\n",
        "# Compute the Mean Squared Error (MSE) between true values and predicted values\n",
        "mse = np.mean((Y_pred - Y_test) ** 2)\n",
        "print(f\"Mean Squared Error on Test Data: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du9Qd72Mg_vP",
        "outputId": "8e4a57a0-7251-41e5-fc27-c6f287c3b3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Data: 0.16170410037842894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation='relu'):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2. / input_size)  # He initialization\n",
        "        self.biases = np.zeros((output_size, 1))\n",
        "\n",
        "        # Store activation function\n",
        "        self.activation_function = self.get_activation_function(activation)\n",
        "        self.activation_derivative = self.get_activation_derivative(activation)\n",
        "\n",
        "    def get_activation_function(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: np.maximum(0, x)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: 1 / (1 + np.exp(-x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: np.tanh(x)\n",
        "        else:\n",
        "            return lambda x: x  # Linear activation\n",
        "\n",
        "    def get_activation_derivative(self, activation):\n",
        "        if activation == 'relu':\n",
        "            return lambda x: (x > 0).astype(float)\n",
        "        elif activation == 'sigmoid':\n",
        "            return lambda x: self.activation_function(x) * (1 - self.activation_function(x))\n",
        "        elif activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x)**2\n",
        "        else:\n",
        "            return lambda x: np.ones_like(x)  # Derivative for linear is 1\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.z = np.dot(self.weights, input_data) + self.biases\n",
        "        self.a = self.activation_function(self.z)\n",
        "        return self.a\n",
        "\n",
        "    def backward(self, dA, learning_rate):\n",
        "        m = self.input_data.shape[1]  # Batch size\n",
        "        self.dZ = dA * self.activation_derivative(self.z)  # Elementwise multiplication of derivative\n",
        "        self.dW = np.dot(self.dZ, self.input_data.T) / m\n",
        "        self.db = np.sum(self.dZ, axis=1, keepdims=True) / m\n",
        "        dA_prev = np.dot(self.weights.T, self.dZ)\n",
        "\n",
        "        print(\"dW:\", self.dW)\n",
        "        print(\"db:\", self.db)\n",
        "\n",
        "        # Update weights and biases using SGD\n",
        "        self.weights -= learning_rate * self.dW\n",
        "        self.biases -= learning_rate * self.db\n",
        "\n",
        "        return dA_prev\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size):\n",
        "        self.layers = []\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, X):\n",
        "        input_data = X\n",
        "        for layer in self.layers:\n",
        "            input_data = layer.forward(input_data)\n",
        "        return input_data\n",
        "\n",
        "    def backward(self, Y, learning_rate):\n",
        "        m = Y.shape[1]\n",
        "        dA = self.layers[-1].a - Y  # Loss derivative with respect to output\n",
        "        for layer in reversed(self.layers):\n",
        "            dA = layer.backward(dA, learning_rate)\n",
        "\n",
        "    def compute_loss(self, predictions, Y):\n",
        "        m = Y.shape[1]\n",
        "        return np.sum((predictions - Y) ** 2) / (2 * m)\n",
        "\n",
        "def target_function(x, y):\n",
        "    return np.sin(2 * np.pi * x * y) + 2 * x * y ** 2\n",
        "\n",
        "nn = NeuralNetwork(input_size=2)\n",
        "nn.add_layer(Layer(input_size=2, output_size=10, activation='relu'))  # First hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=10, activation='relu'))  # Second hidden layer\n",
        "nn.add_layer(Layer(input_size=10, output_size=1, activation='linear'))  # Output layer\n",
        "\n",
        "x = np.random.rand()\n",
        "y = np.random.rand()\n",
        "\n",
        "input_vector = np.array([x, y]).reshape(2, 1)\n",
        "output_vector = nn.forward(input_vector)\n",
        "nn.backward(output_vector, learning_rate=0.01)"
      ],
      "metadata": {
        "id": "GButZ40LhADq"
      },
      "execution_count": 232,
      "outputs": []
    }
  ]
}